<html>

<head>
	<title>Technology Risks | 1107</title>
	<link rel="stylesheet" href="../styles/main.css">
	<script src="../scripts/main.js"></script>
</head>

<body>
	<h1>Technology Risks</h1>
	<nav></nav>

	<p>
		Autonomous vehicles require a method of prioritisation for what to do in attempts to avoid accidents.
		This leads to unavoidable risks as the AI must decide which vehicle or person is the least likely to be damaged or injured when an incident occurs and
		the vehicle must make an immediate correction.
		Autonomous systems cannot be excused for making incorrect decisions like people can as the AI has enough time to make the legally and morally best
		decision (Kallioinen et al., 2019, section 1).
		This brings up ethical concerns regarding potential biases in the determination of these priorities.
	</p>
	<p>
		Traditional vehicles and self-driving car tests are all driven with a human driver who is solely responsible for following the regulations and control
		the vehicle at all times.
		When the point is reached where cars can be driven entirely without a driver the critical unanswered question regarding who would be at fault in an
		accident must be resolved.
		There is no obvious entity to blame in such a scenario
		(Hansson, Belin, & Lundgren, 2021, “Responsibility for Safety” section; Filiz, 2020, 5.2.3).
	</p>
	<p>
		Autonomous vehicles will be using software and so much receive software updates from the internet.
		There are two problems with this.
		The first is that software updates may contain bugs or improperly tested code, and an internet connection makes the software susceptible to hacking
		(Filiz, 2020, 5.2.1).
	</p>

	<footer></footer>
</body>

</html>
